{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vpdLE7wJ-4IL",
        "outputId": "18aa182a-a346-49b9-f1b1-f728c04375e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.2.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: langchain-core>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (0.3.72)\n",
            "Collecting chromadb>=1.0.9 (from langchain_chroma)\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.2.2.post1)\n",
            "Collecting pybase64>=1.4.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.74.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (8.5.0)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.25.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain_chroma) (0.4.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.70->langchain_chroma) (1.33)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.70->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.26.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.3.1)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain_chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.70->langchain_chroma) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.9->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (2.19.2)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (0.1.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.6.1)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.2.5-py3-none-any.whl (12 kB)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score, pypika\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0f8148a62553b9700e621547a244d8c5b4aa12961d1b1d97376f8846c6e07303\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=8a49014838226ae0157a9f9da9c32b6c524aa03cc8fc82447c9ebe51de110a94\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built rouge_score pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, rouge_score, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, langchain_huggingface, evaluate, chromadb, langchain_chroma\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 evaluate-0.4.5 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 langchain_chroma-0.2.5 langchain_huggingface-0.3.1 mmh3-5.2.0 onnxruntime-1.22.1 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 python-dotenv-1.1.1 rouge_score-0.1.2 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -q gradio transformers evaluate rouge_score datasets nltk langchain_chroma langchain_huggingface mauve-text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEHLoXpO7r63"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import mauve\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4AQTIaQ2WMN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load embedding model\n",
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Download and load the Chroma database\n",
        "def load_vector_db():\n",
        "    chroma_db_path = hf_hub_download(\n",
        "        repo_id=\"Ola1mohammed/GovernAI_dataset\",\n",
        "        filename=\"chroma_db.zip\",\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "    extract_dir = \"./chroma_db\"\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(chroma_db_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    return Chroma(persist_directory=extract_dir, embedding_function=embed_model)\n",
        "\n",
        "vector_db = load_vector_db()\n",
        "TOGETHER_API_KEY = \"Token_Here\"  # Replace with your actual API key\n",
        "\n",
        "# Load API key from environment variables\n",
        "TOGETHER_API_URL = \"https://api.together.xyz/v1/completions\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "7bHsKGg_-2dF",
        "outputId": "9b50a457-6a6e-4e8d-bdbd-f3aeab69ed4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-7-939411477.py:206: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Chat History\", bubble_full_width=False)\n",
            "/tmp/ipython-input-7-939411477.py:206: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(label=\"Chat History\", bubble_full_width=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rouge1: 0.12\n",
            "rouge2: 0.02\n",
            "rougeL: 0.10\n",
            "rougeLsum: 0.11\n",
            "F1 Score: 0.02\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cbbbb6606af05bc0b1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://cbbbb6606af05bc0b1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if not TOGETHER_API_KEY:\n",
        "    raise ValueError(\"Missing Together AI API key. Set it in your environment.\")\n",
        "\n",
        "# Function to retrieve context from the vector database\n",
        "def query_rag_system(question):\n",
        "    best_doc_list = vector_db.similarity_search(question, k=1)\n",
        "    if not best_doc_list:  # Check if the list is empty\n",
        "        return \"No relevant context found.\", None, []  # Return a default message and empty lists\n",
        "\n",
        "    best_doc = best_doc_list[0]\n",
        "    additional_docs = vector_db.similarity_search(question, k=3)\n",
        "    additional_docs = [doc for doc in additional_docs if doc != best_doc]\n",
        "\n",
        "    context = f\"Best Chunk:\\n{best_doc.page_content}\\n\\nAdditional Context:\\n\" + \"\\n\".join(doc.page_content for doc in additional_docs)\n",
        "    return context, best_doc, additional_docs\n",
        "\n",
        "# Function to call Together AI for generating a response\n",
        "def query_together_ai(prompt):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7,\n",
        "    }\n",
        "    response = requests.post(TOGETHER_API_URL, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()[\"choices\"][0][\"text\"]\n",
        "    return \"Error: Unable to generate a response from Together AI.\"\n",
        "\n",
        "# Function to process chat interactions\n",
        "def chat_interface(question, chat_history):\n",
        "    if not question:\n",
        "        return chat_history\n",
        "\n",
        "    # Retrieve RAG context\n",
        "    context, best_doc, additional_docs = query_rag_system(question)\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI and Data Governance Compliance Agent. Your task is to analyze and classify the given dataset based on the most relevant principle. Follow these steps:\n",
        "\n",
        "    1. **Identify the Relevant Principle:** Determine which principle applies best to the given query.\n",
        "    2. **Provide a Clear and Concise Answer:** Explain the classification or guideline using simple, precise language.\n",
        "    3. **Justify Your Answer:** Support your response with a logical explanation based on risk, access control, and best practices.\n",
        "\n",
        "    Here are some classification principles you should consider:\n",
        "\n",
        "    - **Open by Default**: Determine if the dataset should be publicly accessible unless its nature requires higher security.\n",
        "    - **Necessity and Proportionality**: Classify the dataset based on its necessity and sensitivity.\n",
        "    - **Timely Classification**: Explain when the dataset should be classified and why timely classification is crucial.\n",
        "    - **Highest Level of Protection**: Assess if the dataset requires a top-level security classification.\n",
        "    - **Segregation of Duties**: Explain the role separation needed for handling the dataset.\n",
        "    - **Need to Know**: Identify who should have access based on their role and necessity.\n",
        "    - **Least Privilege**: Ensure access is granted only at the minimal required level.\n",
        "    - **Data Classification Levels**: Categorize the dataset into Top Secret, Secret, Restricted, or Public.\n",
        "    - **Restricted Sub-Levels**: Determine the appropriate restriction category (A, B, or C) based on its impact.\n",
        "    - **Impact Assessment**: Analyze the risks of unauthorized disclosure.\n",
        "    - **Data Classification Controls**: Recommend security measures such as encryption, retention policies, and access control.\n",
        "    - **Data Classification Process**: Outline the steps for proper classification.\n",
        "    - **Roles and Responsibilities**: Describe the responsibilities of individuals involved in data classification.\n",
        "\n",
        "    **Dataset / Query:**\n",
        "    {question}\n",
        "\n",
        "    **Context:**\n",
        "    {context}\n",
        "\n",
        "    Use the following context to answer the question concisely and do not repeat any phrases:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    **Answer:**\n",
        "    \"\"\"\n",
        "\n",
        "    # Get response from Together AI\n",
        "    response_text = query_together_ai(prompt)\n",
        "    if best_doc: # Check if best_doc is not None\n",
        "        sources = set(doc.metadata[\"category\"] for doc in [best_doc] + additional_docs)\n",
        "        final_response = f\"{response_text}\\n\\nğŸ“Œ **Sources**: {', '.join(sources)}\"\n",
        "    else:\n",
        "        final_response = response_text + \"\\n\\nğŸ“Œ **Sources**: No relevant sources found.\"\n",
        "\n",
        "\n",
        "    # Append messages with user on the right and chatbot on the left\n",
        "    chat_history.append((question, None))  # User message on right\n",
        "    chat_history.append((None, final_response))  # Chatbot message on left\n",
        "    return chat_history\n",
        "\n",
        "# Helper function for evaluation to generate a response using the same pipeline\n",
        "def generate_response(question):\n",
        "    context, best_doc, additional_docs = query_rag_system(question)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI and Data Governance Compliance Agent. Your task is to analyze and classify the given dataset based on the most relevant principle. Follow these steps:\n",
        "\n",
        "    1. **Identify the Relevant Principle:** Determine which principle applies best to the given query.\n",
        "    2. **Provide a Clear and Concise Answer:** Explain the classification or guideline using simple, precise language.\n",
        "    3. **Justify Your Answer:** Support your response with a logical explanation based on risk, access control, and best practices.\n",
        "\n",
        "    **Dataset / Query:**\n",
        "    {question}\n",
        "\n",
        "    **Context:**\n",
        "    {context}\n",
        "\n",
        "    Use the following context to answer the question concisely and do not repeat any phrases:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    **Answer:**\n",
        "    \"\"\"\n",
        "    return query_together_ai(prompt).strip()\n",
        "\n",
        "# Function to evaluate the RAG system\n",
        "def evaluate_rag_system():\n",
        "    # Test set\n",
        "    test_questions = [\n",
        "        \"What are the key data classification principles in our organization?\",\n",
        "        \"How should we handle integrated datasets with mixed classification levels?\",\n",
        "        \"What is the 'Open by Default' principle and how is it applied differently across sectors?\",\n",
        "        \"When should data classification be timebound?\",\n",
        "        \"How does the 'Segregation of Duties' principle affect data handling responsibilities?\",\n",
        "        \"What factors determine the classification level according to the 'Necessity and Proportionality' principle?\"\n",
        "    ]\n",
        "\n",
        "    test_refs = [\n",
        "        \"The key data classification principles include: Open by Default, Necessity and Proportionality, Timely Classification, Highest Level of Protection, and Segregation of Duties.\",\n",
        "        \"According to Principle 4 (Highest Level of Protection), if information includes an integrated dataset with different classification levels, the highest classification level shall be approved.\",\n",
        "        \"The Open by Default principle states that data shall primarily be accessible in the development sector unless its sensitivity requires higher protection, and top secret in political and security sectors unless its sensitivity requires lower protection.\",\n",
        "        \"According to Principle 3 (Timely Classification), data shall be classified upon creation or receipt from other entities, and said classification should be timebound.\",\n",
        "        \"The Segregation of Duties principle requires that worker responsibilities related to data classification, access, disclosure, use, modification, or destruction shall be segregated to prevent overlap of powers and avoid dispersal of responsibilities.\",\n",
        "        \"According to the Necessity and Proportionality principle, data shall be classified based on its nature, sensitivity, and impact, balancing its value against its confidentiality level.\"\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Generate predictions\n",
        "    pred_responses = [generate_response(question) for question in test_questions]\n",
        "\n",
        "    # BLEU Score\n",
        "    bleu = evaluate.load(\"bleu\")\n",
        "    bleu_results = bleu.compute(predictions=pred_responses, references=[[ref] for ref in test_refs])\n",
        "    # print(f\"BLEU Score: {bleu_results['bleu']:.2f}\")\n",
        "\n",
        "    # ROUGE Score\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    rouge_results = rouge.compute(predictions=pred_responses, references=test_refs)\n",
        "    for key, value in rouge_results.items():\n",
        "        print(f\"{key}: {value:.2f}\")\n",
        "\n",
        "    # F1 Score\n",
        "    pred_tokens = [word_tokenize(response) for response in pred_responses]\n",
        "    ref_tokens = [word_tokenize(ref) for ref in test_refs]\n",
        "    pred_flat = [token for sublist in pred_tokens for token in sublist]\n",
        "    ref_flat = [token for sublist in ref_tokens for token in sublist]\n",
        "\n",
        "    # Align the lengths of the token lists\n",
        "    min_length = min(len(pred_flat), len(ref_flat))\n",
        "    pred_flat = pred_flat[:min_length]\n",
        "    ref_flat = ref_flat[:min_length]\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(ref_flat, pred_flat, average='weighted')\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Welcome message\n",
        "welcome_message = \"\"\"\n",
        "ğŸ‘‹ Welcome to **GovernAI** â€” your intelligent assistant for Data & AI Governance!\n",
        "\n",
        "Ask me anything about data classification principles, governance policies, access control, or compliance best practices.\n",
        "I'm here to help you interpret and apply regulatory guidelines clearly and effectively.\n",
        "\n",
        "Start by typing your question below â¬‡ï¸\n",
        "\"\"\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# **GovernAI** Chatbot\")\n",
        "    gr.Markdown(welcome_message)\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat History\", bubble_full_width=False)\n",
        "    question = gr.Textbox(lines=2, placeholder=\"Type your question...\", label=\"Your Question\", value=\"\")\n",
        "    submit_button = gr.Button(\"Send\")\n",
        "    chat_history = gr.State([])\n",
        "\n",
        "    submit_button.click(fn=chat_interface, inputs=[question, chat_history], outputs=chatbot)\n",
        "\n",
        "# Run the evaluation\n",
        "print(\"Running evaluation...\")\n",
        "evaluate_rag_system()\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
